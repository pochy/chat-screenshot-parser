#!/usr/bin/env python3
"""
ä¸­å›½èªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«æ—¥æœ¬èªç¿»è¨³ã‚’è¿½åŠ ã™ã‚‹ãƒ„ãƒ¼ãƒ«

ãƒ­ãƒ¼ã‚«ãƒ«LLM(Ollamaç­‰)ã¾ãŸã¯ãƒãƒƒãƒå‡¦ç†ç”¨

ä½¿ç”¨æ–¹æ³•:
    # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†
    # Ollamaä½¿ç”¨(qwen2ç­‰ã®ä¸­å›½èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«æ¨å¥¨)
    python translate.py --input ./output/conversations.jsonl --output ./output/translated.jsonl --backend ollama --model qwen2.5:7b

    # Ollamaè©³ç´°ç¿»è¨³(å˜èªè§£èª¬ãƒ»ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹åˆ†æãƒ»è¿”ä¿¡æ¡ˆã‚’å«ã‚€)
    python translate.py --input ./output/conversations.jsonl --output ./output/translated.jsonl --backend ollama --model qwen2.5:7b --detailed

    # Geminié€šå¸¸API(ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ç¿»è¨³)
    python translate.py --input ./output/conversations.jsonl --output ./output/translated.jsonl --backend gemini --model gemini-2.0-flash

    # Geminiè©³ç´°ç¿»è¨³(å˜èªè§£èª¬ãƒ»ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹åˆ†æãƒ»è¿”ä¿¡æ¡ˆã‚’å«ã‚€)
    python translate.py --input ./output/conversations.jsonl --output ./output/translated.jsonl --backend gemini --model gemini-2.0-flash --detailed

    # Gemini ãƒãƒƒãƒAPI(50%å‰²å¼•ã€éåŒæœŸå‡¦ç†)
    python translate.py --input ./output/conversations.jsonl --output ./output/translated.jsonl --backend gemini-batch --model gemini-2.0-flash

    # ç¿»è¨³ãªã—ã§ text_ja ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã ã‘è¿½åŠ (å¾Œã§æ‰‹å‹•/ä»–ãƒ„ãƒ¼ãƒ«ã§ç¿»è¨³)
    python translate.py --input ./output/conversations.jsonl --output ./output/with_text_ja.jsonl --backend none
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‡¦ç†ï¼ˆæ—¥æ¯ã«åˆ†å‰²ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬å‡¦ç†ï¼‰
    # --count ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯æ—¥æ•°ã§åˆ¶é™ï¼ˆä¾‹: --count 10 ã§æœ€åˆã®10æ—¥åˆ†ã‚’å‡¦ç†ï¼‰
    python translate.py --input-dir ./output/daily --output-dir ./output/translated --backend ollama --model qwen2.5:7b
    python translate.py --input-dir ./output/daily --output-dir ./output/translated --backend ollama --model qwen2.5:7b --count 10
"""

import argparse
import json
import sys
import time
from pathlib import Path
from typing import Optional, Dict, List, Tuple
from tqdm import tqdm
import os
import requests
import re
import tempfile
from dotenv import load_dotenv

# .env ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã¿
load_dotenv()


def sanitize_text_for_prompt(text: str) -> str:
    """
    ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¤ãƒ³ã‚¸ã‚§ã‚¯ã‚·ãƒ§ãƒ³å¯¾ç­–: ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚µãƒ‹ã‚¿ã‚¤ã‚º

    Args:
        text: å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ

    Returns:
        ã‚µãƒ‹ã‚¿ã‚¤ã‚ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
    """
    # åˆ¶å¾¡æ–‡å­—ã‚’é™¤å»
    text = re.sub(r'[\x00-\x1F\x7F]', '', text)

    # é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã¯åˆ‡ã‚Šè©°ã‚ï¼ˆ5000æ–‡å­—ã¾ã§ï¼‰
    if len(text) > 5000:
        text = text[:5000]

    return text


def get_available_models() -> list:
    """Ollamaã§åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—"""
    try:
        import requests

        response = requests.get("http://localhost:11434/api/tags", timeout=10)
        if response.status_code == 200:
            models = response.json().get("models", [])
            return [model["name"] for model in models]
    except Exception as e:
        print(f"ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)

    return []


def translate_with_ollama(text: str, model: str = "qwen2.5:7b", timeout: int = 180) -> Optional[str]:
    """Ollamaã§ç¿»è¨³"""
    try:
        import requests

        # ãƒ†ã‚­ã‚¹ãƒˆã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º
        text = sanitize_text_for_prompt(text)

        # ãƒ¢ãƒ‡ãƒ«å­˜åœ¨ç¢ºèª
        available_models = get_available_models()
        if available_models and model not in available_models:
            print(f"è­¦å‘Š: ãƒ¢ãƒ‡ãƒ« '{model}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", file=sys.stderr)
            print(f"åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {available_models}", file=sys.stderr)

            # ä»£æ›¿ãƒ¢ãƒ‡ãƒ«ã‚’ææ¡ˆ
            chinese_models = [m for m in available_models if any(keyword in m.lower()
                            for keyword in ['qwen', 'chatglm', 'baichuan', 'llama2-chinese'])]
            if chinese_models:
                suggested_model = chinese_models[0]
                print(f"ä¸­å›½èªå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®å€™è£œ: {suggested_model}", file=sys.stderr)

        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model,
                "prompt": f"å°†ä»¥ä¸‹ä¸­æ–‡ç¿»è¯‘æˆæ—¥è¯­ï¼Œåªè¾“å‡ºç¿»è¯‘ç»“æœï¼š\n{text}",
                "stream": False,
            },
            timeout=timeout
        )
        
        if response.status_code == 200:
            result = response.json().get("response", "").strip()
            print(f"ç¿»è¨³æˆåŠŸ: {text[:20]}... -> {result[:20]}...")
            return result
        else:
            print(f"Ollamaã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}", file=sys.stderr)
            
    except Exception as e:
        print(f"ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
    
    
    return None


def translate_with_ollama_detailed(
    text: str,
    model: str = "qwen2.5:7b",
    timeout: int = 300
) -> Optional[str]:
    """Ollamaã§è©³ç´°ç¿»è¨³ï¼ˆå˜èªåˆ†è§£ã€ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹åˆ†æã€è¿”ä¿¡æ¡ˆã‚’å«ã‚€ï¼‰

    Args:
        text: ç¿»è¨³ã™ã‚‹ä¸­å›½èªãƒ†ã‚­ã‚¹ãƒˆ
        model: ä½¿ç”¨ã™ã‚‹Ollamaãƒ¢ãƒ‡ãƒ«
        timeout: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•°ï¼ˆè©³ç´°ç¿»è¨³ã¯æ™‚é–“ãŒã‹ã‹ã‚‹ãŸã‚é•·ã‚ã«è¨­å®šï¼‰

    Returns:
        Markdownå½¢å¼ã®è©³ç´°è§£èª¬ã€å¤±æ•—æ™‚ã¯None
    """
    try:
        import requests

        # ãƒ†ã‚­ã‚¹ãƒˆã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º
        text = sanitize_text_for_prompt(text)

        # ãƒ¢ãƒ‡ãƒ«å­˜åœ¨ç¢ºèª
        available_models = get_available_models()
        if available_models and model not in available_models:
            print(f"è­¦å‘Š: ãƒ¢ãƒ‡ãƒ« '{model}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", file=sys.stderr)
            print(f"åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«: {available_models}", file=sys.stderr)

        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
        prompt = DETAILED_TRANSLATION_PROMPT.format(text=text)

        response = requests.post(
            "http://localhost:11434/api/generate",
            json={
                "model": model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": 0.3,  # è¿”ä¿¡æ¡ˆç”Ÿæˆã®ãŸã‚è‹¥å¹²é«˜ã‚
                    "num_predict": 4096  # è©³ç´°ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ãŸã‚é•·ã‚ã«è¨­å®š
                }
            },
            timeout=timeout
        )

        if response.status_code == 200:
            detailed_translation = response.json().get("response", "").strip()

            # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
            if validate_detailed_response(detailed_translation):
                print(f"è©³ç´°ç¿»è¨³æˆåŠŸ: {text[:20]}... ({len(detailed_translation)} chars)")
                return detailed_translation
            else:
                print(f"è­¦å‘Š: ä¸€éƒ¨ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒæ¬ è½ã—ã¦ã„ã¾ã™: {text[:20]}...", file=sys.stderr)
                # éƒ¨åˆ†çš„ãªçµæœã§ã‚‚è¿”ã™
                return detailed_translation
        else:
            print(f"Ollamaã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}", file=sys.stderr)
            return None

    except Exception as e:
        print(f"è©³ç´°ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        return None


def confirm_translation(messages: List[dict], detailed: bool = False, model: str = "gemini-2.0-flash") -> bool:
    """ç¿»è¨³å®Ÿè¡Œã®ç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¡¨ç¤º

    Args:
        messages: ç¿»è¨³å¯¾è±¡ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ
        detailed: è©³ç´°ç¿»è¨³ãƒ¢ãƒ¼ãƒ‰ã‹ã©ã†ã‹
        model: ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«

    Returns:
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç¶šè¡Œã‚’é¸æŠã—ãŸå ´åˆTrue
    """
    # ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºè¨ˆç®—
    data_size = calculate_data_size(messages)

    # ã‚³ã‚¹ãƒˆæ¨å®š
    avg_chars = sum(len(m.get("text", "")) for m in messages) / len(messages) if messages else 0
    if detailed:
        cost_estimate = estimate_detailed_cost(len(messages), int(avg_chars))
        mode_str = "è©³ç´°ç¿»è¨³"
    else:
        cost_estimate = estimate_simple_cost(len(messages), int(avg_chars))
        mode_str = "ç°¡æ˜“ç¿»è¨³"

    # ç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆè¡¨ç¤º
    print("\n" + "="*60)
    print(f"ã€{mode_str}ãƒ¢ãƒ¼ãƒ‰ã€‘å‡¦ç†å®Ÿè¡Œã®ç¢ºèª")
    print("="*60)
    print(f"ãƒ¢ãƒ‡ãƒ«: {model}")
    print(f"é€ä¿¡ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {len(messages)}ä»¶")
    print(f"é€ä¿¡ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º: {data_size['size_str']}")
    print(f"æ¨å®šæ–™é‡‘: ${cost_estimate['estimated_cost_usd']} (ç´„{cost_estimate['estimated_cost_jpy']}å††)")
    print(f"æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³: å…¥åŠ› {cost_estimate['estimated_input_tokens']}, å‡ºåŠ› {cost_estimate['estimated_output_tokens']}")
    print("="*60)

    # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
    try:
        response = input("ç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ [Y/n]: ").strip().lower()
        if response == '' or response == 'y' or response == 'yes':
            print("å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™...\n")
            return True
        else:
            print("å‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚")
            return False
    except (KeyboardInterrupt, EOFError):
        print("\nå‡¦ç†ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸã€‚")
        return False


def translate_with_gemini(text: str, api_key: str, model: str = "gemini-1.5-flash") -> Optional[str]:
    """Gemini APIã§ç¿»è¨³"""
    # ãƒ†ã‚­ã‚¹ãƒˆã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º
    text = sanitize_text_for_prompt(text)

    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"

    headers = {
        "Content-Type": "application/json",
        "x-goog-api-key": api_key
    }

    prompt = f"Translate the following Chinese text into natural Japanese. Only output the translated text.\n\nText: {text}"
    
    data = {
        "contents": [{
            "parts": [{"text": prompt}]
        }],
        "generationConfig": {
            "temperature": 0.1
        }
    }
    
    try:
        response = requests.post(url, headers=headers, json=data, timeout=30)
        
        if response.status_code == 200:
            result_json = response.json()
            try:
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ : candidates[0].content.parts[0].text
                translation = result_json["candidates"][0]["content"]["parts"][0]["text"].strip()
                print(f"ç¿»è¨³æˆåŠŸ: {text[:20]}... -> {translation[:20]}...")
                return translation
            except (KeyError, IndexError) as e:
                print(f"Geminiãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
                return None
        else:
            print(f"Gemini APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}", file=sys.stderr)
            return None
            
    except Exception as e:
        print(f"ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        return None


# è©³ç´°ç¿»è¨³ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
DETAILED_TRANSLATION_PROMPT = """ã‚ãªãŸã¯æ—¥æœ¬äººå­¦ç¿’è€…ã‚’æ”¯æ´ã™ã‚‹ä¸­å›½èªæ•™å¸«ã§ã™ã€‚ä¼šè©±ç›¸æ‰‹ã‹ã‚‰å—ã‘å–ã£ãŸä¸­å›½èªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã€åŒ…æ‹¬çš„ã«è§£èª¬ã—ã¦ãã ã•ã„ã€‚

**é‡è¦ï¼šã¾ãšåŸæ–‡ã«èª¤å­—ãƒ»è„±å­—ãŒãªã„ã‹ç¢ºèªã—ã€ã‚‚ã—èª¤ã‚ŠãŒã‚ã‚Œã°ä¿®æ­£ã—ã¦ã‹ã‚‰è§£èª¬ã‚’é€²ã‚ã¦ãã ã•ã„ã€‚**

ä»¥ä¸‹ã®ä¸­å›½èªãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã—ã€ä¸‹è¨˜ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æ—¥æœ¬èªã§èª¬æ˜ã—ã¦ãã ã•ã„ï¼š

## åŸæ–‡

{text}

**èª¤å­—ãŒã‚ã‚‹å ´åˆã®ã¿ï¼š**
â€»è‡ªç„¶ãªä¸­å›½èªï¼š**[ä¿®æ­£å¾Œã®æ­£ã—ã„ä¸­å›½èª]**
â€»æ–‡è„ˆä¸Šã€ã€Œ[èª¤å­—]ã€ã§ã¯ãªã **ã€Œ[æ­£ã—ã„å­—]ã€** ã®èª¤å­—ã ã¨åˆ¤æ–­ã—ã¾ã™ã€‚

---

## æ—¥æœ¬èªã®æ„å‘³ï¼ˆè‡ªç„¶è¨³ï¼‰

è‡ªç„¶ã§æµæš¢ãªæ—¥æœ¬èªè¨³ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚ç›´è¨³ã§ã¯ãªãã€æ—¥æœ¬èªã¨ã—ã¦è‡ªç„¶ãªè¡¨ç¾ã«ã—ã¦ãã ã•ã„ã€‚
**èª¤å­—ãŒã‚ã£ãŸå ´åˆã¯ã€ä¿®æ­£å¾Œã®ä¸­å›½èªã‚’åŸºã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚**

---

## ä¸­å›½èªã®åˆ†è§£è§£èª¬

ä»¥ä¸‹ã®ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼ã§ã€ãƒ†ã‚­ã‚¹ãƒˆä¸­ã®é‡è¦ãªå˜èªãƒ»ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’è§£èª¬ã—ã¦ãã ã•ã„ï¼š

| å˜èª | å“è© | ãƒ”ãƒ³ã‚¤ãƒ³ | æ„å‘³ | æ–°HSK | è§£èª¬ |
| :-- | :---- | :---------- | :------- | :--- | :----------- |

**æ³¨æ„ç‚¹ï¼š**
- ãƒ†ã‚­ã‚¹ãƒˆä¸­ã®ä¸»è¦ãªå˜èªãƒ»æ…£ç”¨å¥ã‚’å…¨ã¦å«ã‚ã‚‹ã“ã¨
- æ–°HSKãƒ¬ãƒ™ãƒ«ï¼ˆ1-9ï¼‰ã‚’æ­£ç¢ºã«è¨˜è¼‰
- ã€Œè§£èª¬ã€åˆ—ã§ã¯ã€ã“ã®æ–‡è„ˆã§ã®å½¹å‰²ã‚„æ„Ÿæƒ…çš„ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹ã‚’èª¬æ˜
- èª¤å­—ãŒã‚ã£ãŸå ´åˆã¯ã€æ­£ã—ã„å˜èªã‚’è§£èª¬ã«å«ã‚ã‚‹ã“ã¨

**èª¤å­—ãŒã‚ã‚‹å ´åˆã®ã¿ï¼š**
ğŸ” **è£œè¶³**
* ã€Œ[èª¤å­—ã‚’å«ã‚€è¡¨ç¾]ã€ï¼[ä¸è‡ªç„¶ãªæ„å‘³]ã€ã¨ã„ã†æ„å‘³ã«ãªã‚Šä¸è‡ªç„¶

---

## å…¨ä½“ã®ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹

ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å…¨ä½“ã®æ„Ÿæƒ…çš„ãƒˆãƒ¼ãƒ³ã€äºŒäººã®é–¢ä¿‚æ€§ã€æ–‡åŒ–çš„èƒŒæ™¯ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚

**åˆ†æãƒã‚¤ãƒ³ãƒˆï¼š**
- æ„Ÿæƒ…çš„ãƒˆãƒ¼ãƒ³ï¼ˆå„ªã—ã„ã€å¿ƒé…ã€æ¥½ã—ã„ã€ãªã©ï¼‰
- é–¢ä¿‚æ€§ï¼ˆæ‹äººã€å‹äººã€è¦ªå¯†åº¦ãªã©ï¼‰
- æ–‡åŒ–çš„ãƒ»ç¤¾ä¼šçš„èƒŒæ™¯
- è¾¼ã‚ã‚‰ã‚ŒãŸæ„å›³ã‚„æœŸå¾…

ç®‡æ¡æ›¸ãã§ã€é‡è¦ãªç‚¹ã‚’**å¤ªå­—**ã§å¼·èª¿ã—ã¦ãã ã•ã„ã€‚

---

## æ—¥æœ¬èªã§ã®è¿”äº‹æ¡ˆï¼ˆ3ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰

ç›¸æ‰‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«å¯¾ã™ã‚‹è¿”ç­”ä¾‹ã‚’ã€3ã¤ã®ç•°ãªã‚‹å£èª¿ã§ææ¡ˆã—ã¦ãã ã•ã„ï¼š

### 1. **è¦ªè¿‘æ„ŸUPæ¡ˆ**

**è¿”ä¿¡ä¾‹ï¼š**
[è¦ªå¯†ã§æ€ã„ã‚„ã‚Šã®ã‚ã‚‹è¿”ç­”]
**ç†ç”±ï¼š**
[ãªãœã“ã®è¿”ç­”ãŒåŠ¹æœçš„ã‹]

---

### 2. **ãƒ¦ãƒ¼ãƒ¢ã‚¢æ¡ˆ**

**è¿”ä¿¡ä¾‹ï¼š**
[æ˜ã‚‹ãã‚«ã‚¸ãƒ¥ã‚¢ãƒ«ãªè¿”ç­”]
**ç†ç”±ï¼š**
[ãªãœã“ã®è¿”ç­”ãŒåŠ¹æœçš„ã‹]

---

### 3. **èª å®Ÿãƒ»å„ªã—ã•æ¡ˆ**

**è¿”ä¿¡ä¾‹ï¼š**
[ç›¸æ‰‹ã‚’å®‰å¿ƒã•ã›ã‚‹åŒ…å®¹åŠ›ã®ã‚ã‚‹è¿”ç­”]
**ç†ç”±ï¼š**
[ãªãœã“ã®è¿”ç­”ãŒåŠ¹æœçš„ã‹]

---

**é‡è¦ãªæŒ‡ç¤ºï¼š**
- åŸæ–‡ã«èª¤å­—ãŒã‚ã‚‹å ´åˆã¯ã€å¿…ãšã€ŒåŸæ–‡ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ä¿®æ­£ç‰ˆã‚’ç¤ºã™ã“ã¨
- èª¤å­—ãŒãªã„å ´åˆã¯ã€ä¿®æ­£ã«é–¢ã™ã‚‹è¨˜è¿°ã‚’çœç•¥ã™ã‚‹ã“ã¨
- ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ãƒ†ãƒ¼ãƒ–ãƒ«ã¯æ­£ã—ã„å½¢å¼ï¼ˆ|ã§åŒºåˆ‡ã‚Šã€:--ã§å·¦æƒãˆï¼‰ã§å‡ºåŠ›
- å…¨ã¦ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’å¿…ãšå«ã‚ã‚‹ã“ã¨
- æ—¥æœ¬èªã¯è‡ªç„¶ã§èª­ã¿ã‚„ã™ã„è¡¨ç¾ã‚’ä½¿ç”¨
- ä¸­å›½èªå­¦ç¿’è€…ã®è¦–ç‚¹ã§ã€å…·ä½“çš„ã§å®Ÿç”¨çš„ãªè§£èª¬ã‚’æä¾›
"""


def validate_detailed_response(response: str) -> bool:
    """è©³ç´°ç¿»è¨³ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«å¿…è¦ãªã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒå…¨ã¦å«ã¾ã‚Œã¦ã„ã‚‹ã‹æ¤œè¨¼"""
    required_sections = [
        "## åŸæ–‡",
        "## æ—¥æœ¬èªã®æ„å‘³ï¼ˆè‡ªç„¶è¨³ï¼‰",
        "## ä¸­å›½èªã®åˆ†è§£è§£èª¬",
        "## å…¨ä½“ã®ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹",
        "## æ—¥æœ¬èªã§ã®è¿”äº‹æ¡ˆ"
    ]
    return all(section in response for section in required_sections)


def calculate_data_size(messages: List[dict]) -> dict:
    """ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒ‡ãƒ¼ã‚¿ã®ã‚µã‚¤ã‚ºã‚’è¨ˆç®—

    Args:
        messages: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆ

    Returns:
        ã‚µã‚¤ã‚ºæƒ…å ±ã®è¾æ›¸
    """
    total_bytes = sum(len(m.get("text", "").encode('utf-8')) for m in messages)

    if total_bytes < 1024:
        size_str = f"{total_bytes} B"
    elif total_bytes < 1024 * 1024:
        size_str = f"{total_bytes / 1024:.2f} KB"
    else:
        size_str = f"{total_bytes / (1024 * 1024):.2f} MB"

    return {
        "total_bytes": total_bytes,
        "size_str": size_str
    }


def estimate_simple_cost(message_count: int, avg_chars_per_msg: int = 11) -> dict:
    """ç°¡æ˜“ç¿»è¨³ã®ã‚³ã‚¹ãƒˆã‚’æ¨å®š

    Args:
        message_count: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°
        avg_chars_per_msg: 1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ãŸã‚Šã®å¹³å‡æ–‡å­—æ•°

    Returns:
        ã‚³ã‚¹ãƒˆæ¨å®šæƒ…å ±ã®è¾æ›¸
    """
    # Gemini 2.0 Flash pricing (2025å¹´1æœˆæ™‚ç‚¹)
    input_cost_per_1k = 0.000015   # $0.000015/1K tokens
    output_cost_per_1k = 0.00006   # $0.00006/1K tokens

    # æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°ï¼ˆç°¡æ˜“ç¿»è¨³ï¼‰
    input_tokens_per_msg = 50 + (avg_chars_per_msg * 1.5)  # ç°¡æ˜“ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + ãƒ†ã‚­ã‚¹ãƒˆ
    output_tokens_per_msg = 30  # ç°¡æ˜“ç¿»è¨³ãƒ¬ã‚¹ãƒãƒ³ã‚¹

    total_input_tokens = message_count * input_tokens_per_msg
    total_output_tokens = message_count * output_tokens_per_msg

    input_cost = (total_input_tokens / 1000) * input_cost_per_1k
    output_cost = (total_output_tokens / 1000) * output_cost_per_1k
    total_cost = input_cost + output_cost

    return {
        "message_count": message_count,
        "estimated_input_tokens": int(total_input_tokens),
        "estimated_output_tokens": int(total_output_tokens),
        "estimated_cost_usd": round(total_cost, 4),
        "estimated_cost_jpy": int(total_cost * 160)  # 1ãƒ‰ãƒ«=160å††æ›ç®—
    }


def estimate_detailed_cost(message_count: int, avg_chars_per_msg: int = 20) -> dict:
    """è©³ç´°ç¿»è¨³ã®ã‚³ã‚¹ãƒˆã‚’æ¨å®š

    Args:
        message_count: ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°
        avg_chars_per_msg: 1ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚ãŸã‚Šã®å¹³å‡æ–‡å­—æ•°

    Returns:
        ã‚³ã‚¹ãƒˆæ¨å®šæƒ…å ±ã®è¾æ›¸
    """
    # Gemini 2.0 Flash pricing (2025å¹´1æœˆæ™‚ç‚¹)
    input_cost_per_1k = 0.000015   # $0.000015/1K tokens
    output_cost_per_1k = 0.00006   # $0.00006/1K tokens

    # æ¨å®šãƒˆãƒ¼ã‚¯ãƒ³æ•°
    input_tokens_per_msg = 800 + (avg_chars_per_msg * 1.5)  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + ãƒ†ã‚­ã‚¹ãƒˆ
    output_tokens_per_msg = 2000  # è©³ç´°ãƒ¬ã‚¹ãƒãƒ³ã‚¹

    total_input_tokens = message_count * input_tokens_per_msg
    total_output_tokens = message_count * output_tokens_per_msg

    input_cost = (total_input_tokens / 1000) * input_cost_per_1k
    output_cost = (total_output_tokens / 1000) * output_cost_per_1k
    total_cost = input_cost + output_cost

    return {
        "message_count": message_count,
        "estimated_input_tokens": int(total_input_tokens),
        "estimated_output_tokens": int(total_output_tokens),
        "estimated_cost_usd": round(total_cost, 4),
        "estimated_cost_jpy": int(total_cost * 160)  # 1ãƒ‰ãƒ«=160å††æ›ç®—
    }


def translate_with_gemini_detailed(
    text: str,
    api_key: str,
    model: str = "gemini-2.0-flash"
) -> Optional[str]:
    """Gemini APIã§è©³ç´°ç¿»è¨³ï¼ˆå˜èªåˆ†è§£ã€ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹åˆ†æã€è¿”ä¿¡æ¡ˆã‚’å«ã‚€ï¼‰

    Args:
        text: ç¿»è¨³ã™ã‚‹ä¸­å›½èªãƒ†ã‚­ã‚¹ãƒˆ
        api_key: Google API Key
        model: ä½¿ç”¨ã™ã‚‹Geminiãƒ¢ãƒ‡ãƒ«

    Returns:
        Markdownå½¢å¼ã®è©³ç´°è§£èª¬ã€å¤±æ•—æ™‚ã¯None
    """
    # ãƒ†ã‚­ã‚¹ãƒˆã®ã‚µãƒ‹ã‚¿ã‚¤ã‚º
    text = sanitize_text_for_prompt(text)

    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"

    headers = {
        "Content-Type": "application/json",
        "x-goog-api-key": api_key
    }

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
    prompt = DETAILED_TRANSLATION_PROMPT.format(text=text)

    data = {
        "contents": [{
            "parts": [{"text": prompt}]
        }],
        "generationConfig": {
            "temperature": 0.3  # è¿”ä¿¡æ¡ˆç”Ÿæˆã®ãŸã‚è‹¥å¹²é«˜ã‚
        }
    }

    try:
        response = requests.post(url, headers=headers, json=data, timeout=60)

        if response.status_code == 200:
            result_json = response.json()
            try:
                # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ§‹é€ : candidates[0].content.parts[0].text
                detailed_translation = result_json["candidates"][0]["content"]["parts"][0]["text"].strip()

                # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
                if validate_detailed_response(detailed_translation):
                    print(f"è©³ç´°ç¿»è¨³æˆåŠŸ: {text[:20]}... ({len(detailed_translation)} chars)")
                    return detailed_translation
                else:
                    print(f"è­¦å‘Š: ä¸€éƒ¨ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ãŒæ¬ è½ã—ã¦ã„ã¾ã™: {text[:20]}...", file=sys.stderr)
                    # éƒ¨åˆ†çš„ãªçµæœã§ã‚‚è¿”ã™
                    return detailed_translation

            except (KeyError, IndexError) as e:
                print(f"Geminiãƒ¬ã‚¹ãƒãƒ³ã‚¹è§£æã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
                return None
        else:
            print(f"Gemini APIã‚¨ãƒ©ãƒ¼: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã‚³ãƒ¼ãƒ‰ {response.status_code}", file=sys.stderr)
            return None

    except Exception as e:
        print(f"è©³ç´°ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
        return None


def translate_with_gemini_batch(
    messages: List[dict],
    api_key: str,
    model: str = "gemini-2.0-flash",
    batch_size: int = 100,
    poll_interval: int = 30,
    max_wait_time: int = 86400,
    max_count: int = None
) -> Dict[str, str]:
    """
    Gemini Batch APIã§ä¸€æ‹¬ç¿»è¨³ï¼ˆ50%å‰²å¼•ï¼‰

    Args:
        messages: ç¿»è¨³å¯¾è±¡ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãƒªã‚¹ãƒˆï¼ˆlang=zh, type=textã®ã‚‚ã®ï¼‰
        api_key: Google API Key
        model: ä½¿ç”¨ãƒ¢ãƒ‡ãƒ«
        batch_size: 1ãƒãƒƒãƒã‚ãŸã‚Šã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ï¼ˆã‚¤ãƒ³ãƒ©ã‚¤ãƒ³æ–¹å¼ã®å ´åˆã¯å°ã•ã‚ã«ï¼‰
        poll_interval: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèªé–“éš”ï¼ˆç§’ï¼‰
        max_wait_time: æœ€å¤§å¾…æ©Ÿæ™‚é–“ï¼ˆç§’ï¼‰
        max_count: å‡¦ç†ã™ã‚‹æœ€å¤§ä»¶æ•°ï¼ˆNoneã®å ´åˆã¯å…¨ä»¶å‡¦ç†ï¼‰

    Returns:
        {message_id: translation} ã®è¾æ›¸
    """
    try:
        from google import genai
    except ImportError:
        print("ã‚¨ãƒ©ãƒ¼: google-genai ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ãŒå¿…è¦ã§ã™", file=sys.stderr)
        print("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install google-genai", file=sys.stderr)
        sys.exit(1)

    client = genai.Client(api_key=api_key)
    translations: Dict[str, str] = {}

    # ä¸­å›½èªãƒ†ã‚­ã‚¹ãƒˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ã¿æŠ½å‡º
    zh_messages = [m for m in messages if m.get("lang") == "zh" and m.get("type") == "text"]

    if not zh_messages:
        print("ç¿»è¨³å¯¾è±¡ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒã‚ã‚Šã¾ã›ã‚“")
        return translations

    # ä»¶æ•°åˆ¶é™
    if max_count is not None:
        original_count = len(zh_messages)
        zh_messages = zh_messages[:max_count]
        print(f"ãƒãƒƒãƒç¿»è¨³å¯¾è±¡: {original_count}ä»¶ä¸­ã€æœ€åˆã®{max_count}ä»¶ã‚’å‡¦ç† (ãƒ¢ãƒ‡ãƒ«: {model})")
    else:
        print(f"ãƒãƒƒãƒç¿»è¨³å¯¾è±¡: {len(zh_messages)}ä»¶ (ãƒ¢ãƒ‡ãƒ«: {model})")

    print("ãƒãƒƒãƒAPIã¯é€šå¸¸æ–™é‡‘ã®50%å‰²å¼•ã§ã™")

    # ç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    if not confirm_translation(zh_messages, detailed=False, model=model):
        print("å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã—ãŸã€‚")
        return translations

    # ãƒãƒƒãƒã«åˆ†å‰²ã—ã¦å‡¦ç†
    total_batches = (len(zh_messages) + batch_size - 1) // batch_size

    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(zh_messages))
        batch_messages = zh_messages[start_idx:end_idx]

        print(f"\nãƒãƒƒãƒ {batch_idx + 1}/{total_batches} ã‚’å‡¦ç†ä¸­... ({len(batch_messages)}ä»¶)")

        # ãƒ•ã‚¡ã‚¤ãƒ«å…¥åŠ›æ–¹å¼ã§ãƒãƒƒãƒãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’ä½œæˆ
        requests_data = []
        for m in batch_messages:
            text = sanitize_text_for_prompt(m["text"])
            prompt = f"Translate the following Chinese text into natural Japanese. Only output the translated text.\n\nText: {text}"

            requests_data.append({
                "key": m["id"],
                "request": {
                    "contents": [{"parts": [{"text": prompt}]}],
                    "generationConfig": {"temperature": 0.1}
                }
            })

        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«JSONLå½¢å¼ã§æ›¸ãè¾¼ã¿
        with tempfile.NamedTemporaryFile(mode='w', suffix='.jsonl', delete=False, encoding='utf-8') as f:
            for req in requests_data:
                f.write(json.dumps(req, ensure_ascii=False) + '\n')
            temp_file_path = f.name

        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
            print("ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­...")
            uploaded_file = client.files.upload(
                file=temp_file_path,
                config={"mime_type": "application/jsonl"}
            )
            print(f"ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å®Œäº†: {uploaded_file.name}")

            # ãƒãƒƒãƒã‚¸ãƒ§ãƒ–ã‚’ä½œæˆ
            print("ãƒãƒƒãƒã‚¸ãƒ§ãƒ–ã‚’ä½œæˆä¸­...")
            # ãƒ¢ãƒ‡ãƒ«åã®æ­£è¦åŒ–ï¼ˆ"models/" ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã®é‡è¤‡ã‚’é˜²ãï¼‰
            normalized_model = model.removeprefix("models/")
            batch_job = client.batches.create(
                model=f"models/{normalized_model}",
                src=uploaded_file.name,
                config={"display_name": f"translate-batch-{batch_idx + 1}"}
            )
            print(f"ãƒãƒƒãƒã‚¸ãƒ§ãƒ–ä½œæˆå®Œäº†: {batch_job.name}")

            # ã‚¸ãƒ§ãƒ–å®Œäº†ã‚’å¾…æ©Ÿ
            completed_states = {'JOB_STATE_SUCCEEDED', 'JOB_STATE_FAILED',
                               'JOB_STATE_CANCELLED', 'JOB_STATE_EXPIRED'}

            start_time = time.time()
            with tqdm(desc="ã‚¸ãƒ§ãƒ–å®Œäº†å¾…æ©Ÿä¸­", unit="s") as pbar:
                while True:
                    batch = client.batches.get(name=batch_job.name)
                    state_name = batch.state.name if hasattr(batch.state, 'name') else str(batch.state)

                    if state_name in completed_states:
                        print(f"\nã‚¸ãƒ§ãƒ–å®Œäº†: {state_name}")
                        break

                    elapsed = time.time() - start_time
                    if elapsed > max_wait_time:
                        print(f"\nã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: æœ€å¤§å¾…æ©Ÿæ™‚é–“ ({max_wait_time}ç§’) ã‚’è¶…é")
                        break

                    time.sleep(poll_interval)
                    pbar.update(poll_interval)

            # çµæœã‚’å–å¾—
            if state_name == 'JOB_STATE_SUCCEEDED':
                print("çµæœã‚’å–å¾—ä¸­...")

                # ãƒãƒƒãƒçµ±è¨ˆã‚’è¡¨ç¤º
                if hasattr(batch, 'stats'):
                    stats = batch.stats
                    total = stats.total_request_count if hasattr(stats, 'total_request_count') else 0
                    failed = stats.failed_request_count if hasattr(stats, 'failed_request_count') else 0
                    succeeded = total - failed
                    print(f"  å‡¦ç†å®Œäº†: {succeeded}/{total} ä»¶æˆåŠŸ")
                    if failed > 0:
                        print(f"  âš ï¸  è­¦å‘Š: {failed}/{total} ä»¶å¤±æ•—", file=sys.stderr)

                # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›ã®å ´åˆ
                if hasattr(batch, 'dest') and hasattr(batch.dest, 'file_name') and batch.dest.file_name:
                    result_content = client.files.download(file=batch.dest.file_name)

                    # ãƒã‚¤ãƒˆåˆ—ã®å ´åˆã¯ãƒ‡ã‚³ãƒ¼ãƒ‰
                    if isinstance(result_content, bytes):
                        result_content = result_content.decode('utf-8')

                    for line in result_content.strip().split('\n'):
                        if not line.strip():
                            continue
                        try:
                            result = json.loads(line)

                            # ã‚­ãƒ¼ï¼ˆãƒ¡ãƒƒã‚»ãƒ¼ã‚¸IDï¼‰ã‚’å–å¾—
                            msg_key = result.get("key", "")

                            # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ç¿»è¨³ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
                            if "response" in result:
                                resp = result["response"]
                                if "candidates" in resp and resp["candidates"]:
                                    text_result = resp["candidates"][0]["content"]["parts"][0]["text"].strip()
                                    translations[msg_key] = text_result
                            elif "error" in result:
                                print(f"  ã‚¨ãƒ©ãƒ¼ (ID: {msg_key}): {result['error']}", file=sys.stderr)

                        except (json.JSONDecodeError, KeyError, IndexError) as e:
                            print(f"  çµæœè§£æã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)

                # ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆ
                elif hasattr(batch, 'dest') and hasattr(batch.dest, 'inlined_responses'):
                    for i, resp in enumerate(batch.dest.inlined_responses):
                        if i < len(batch_messages):
                            msg_id = batch_messages[i]["id"]
                            if hasattr(resp, 'response') and resp.response:
                                translations[msg_id] = resp.response.text.strip()

                print(f"ãƒãƒƒãƒ {batch_idx + 1} å®Œäº†: {len([k for k in translations if k in [m['id'] for m in batch_messages]])}ä»¶ç¿»è¨³æˆåŠŸ")

            else:
                print(f"ãƒãƒƒãƒ {batch_idx + 1} å¤±æ•—: {state_name}", file=sys.stderr)
                if hasattr(batch, 'error') and batch.error:
                    print(f"  ã‚¨ãƒ©ãƒ¼è©³ç´°: {batch.error}", file=sys.stderr)

        finally:
            # Google ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒªãƒ¢ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            try:
                client.files.delete(name=uploaded_file.name)
                print(f"ãƒªãƒ¢ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {uploaded_file.name}")
            except Exception as e:
                print(f"ãƒªãƒ¢ãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)

            # ãƒ­ãƒ¼ã‚«ãƒ«ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‰Šé™¤
            try:
                os.unlink(temp_file_path)
            except OSError:
                pass

    print(f"\nå…¨ãƒãƒƒãƒå‡¦ç†å®Œäº†: {len(translations)}/{len(zh_messages)}ä»¶ç¿»è¨³æˆåŠŸ")
    return translations


def translate_batch_for_external(messages: list, output_file: str):
    """
    å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ç”¨ã«ãƒãƒƒãƒç¿»è¨³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
    
    å‡ºåŠ›å½¢å¼ï¼š
    1è¡Œç›®: å…ƒãƒ†ã‚­ã‚¹ãƒˆ
    2è¡Œç›®: (ç©ºè¡Œ - ã“ã“ã«ç¿»è¨³ã‚’å…¥ã‚Œã‚‹)
    3è¡Œç›®: ---
    """
    zh_messages = [m for m in messages if m.get("lang") == "zh" and m.get("type") == "text"]
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for m in zh_messages:
            f.write(f"ID: {m['id']}\n")
            f.write(f"åŸæ–‡: {m['text']}\n")
            f.write(f"ç¿»è¨³: \n")
            f.write("---\n")
    
    print(f"ç¿»è¨³ç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡ºåŠ›: {output_file}")
    print(f"ä¸­å›½èªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {len(zh_messages)}")


def merge_translations(messages: list, translation_file: str) -> list:
    """å¤–éƒ¨ã§ç¿»è¨³ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒ¼ã‚¸"""
    translations = {}
    
    with open(translation_file, 'r', encoding='utf-8') as f:
        content = f.read()
        blocks = content.split("---\n")
        
        for block in blocks:
            if not block.strip():
                continue
            
            lines = block.strip().split("\n")
            msg_id = None
            translation = None
            
            for line in lines:
                if line.startswith("ID: "):
                    msg_id = line[4:].strip()
                elif line.startswith("ç¿»è¨³: "):
                    translation = line[4:].strip()
            
            if msg_id and translation:
                translations[msg_id] = translation
    
    # ãƒãƒ¼ã‚¸
    for m in messages:
        if m["id"] in translations:
            m["text_ja"] = translations[m["id"]]
    
    return messages


def process_single_file(
    input_file: Path,
    output_file: Path,
    backend: str,
    model: str,
    api_key: Optional[str],
    detailed: bool,
    timeout: int,
    batch_size: int,
    poll_interval: int,
    count: Optional[int] = None
) -> Tuple[int, int]:
    """
    å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    
    Returns:
        (å‡¦ç†ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°, ç¿»è¨³ã—ãŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°)
    """
    # ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸èª­ã¿è¾¼ã¿
    messages = []
    with open(input_file, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                messages.append(json.loads(line))
    
    total_messages = len(messages)
    
    if backend == 'export':
        # å¤–éƒ¨ç¿»è¨³ç”¨ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        export_file = str(output_file).replace('.jsonl', '_to_translate.txt')
        translate_batch_for_external(messages, export_file)
        return total_messages, 0
    
    elif backend == 'none':
        # ç¿»è¨³ãªã—ï¼ˆtext_jaãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ç©ºã§è¿½åŠ ï¼‰
        for m in messages:
            if m.get("lang") == "zh" and m.get("type") == "text":
                m["text_ja"] = ""
        translated_count = 0
    
    elif backend == 'ollama':
        # Ollamaã§ç¿»è¨³
        zh_messages = [m for m in messages if m.get("lang") == "zh" and m.get("type") == "text"]
        
        # ä»¶æ•°åˆ¶é™ï¼ˆå˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†æ™‚ã¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã§åˆ¶é™ï¼‰
        if count is not None:
            zh_messages = zh_messages[:count]
        
        if detailed:
            for m in tqdm(zh_messages, desc="è©³ç´°ç¿»è¨³ä¸­", leave=False):
                # ç°¡æ˜“ç¿»è¨³
                translation = translate_with_ollama(m["text"], model, timeout)
                if translation:
                    m["text_ja"] = translation
                
                # è©³ç´°ç¿»è¨³
                detailed_trans = translate_with_ollama_detailed(m["text"], model, timeout)
                if detailed_trans:
                    m["text_ja_detailed"] = detailed_trans
        else:
            for m in tqdm(zh_messages, desc="ç¿»è¨³ä¸­", leave=False):
                translation = translate_with_ollama(m["text"], model, timeout)
                if translation:
                    m["text_ja"] = translation
        
        translated_count = len([m for m in zh_messages if "text_ja" in m])
    
    elif backend == 'gemini':
        # Gemini APIã§ç¿»è¨³
        if not api_key:
            print("ã‚¨ãƒ©ãƒ¼: Geminiã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ --api-key ã¾ãŸã¯ç’°å¢ƒå¤‰æ•° GOOGLE_API_KEY ãŒå¿…è¦ã§ã™", file=sys.stderr)
            sys.exit(1)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«èª¿æ•´
        if model == 'qwen2.5:7b':
            model = 'gemini-2.0-flash'
        
        zh_messages = [m for m in messages if m.get("lang") == "zh" and m.get("type") == "text"]
        
        # ä»¶æ•°åˆ¶é™
        if count is not None:
            zh_messages = zh_messages[:count]
        
        if detailed:
            for m in tqdm(zh_messages, desc="è©³ç´°ç¿»è¨³ä¸­", leave=False):
                # ç°¡æ˜“ç¿»è¨³
                translation = translate_with_gemini(m["text"], api_key, model)
                if translation:
                    m["text_ja"] = translation
                
                # è©³ç´°ç¿»è¨³
                detailed_trans = translate_with_gemini_detailed(m["text"], api_key, model)
                if detailed_trans:
                    m["text_ja_detailed"] = detailed_trans
        else:
            for m in tqdm(zh_messages, desc="ç¿»è¨³ä¸­", leave=False):
                translation = translate_with_gemini(m["text"], api_key, model)
                if translation:
                    m["text_ja"] = translation
        
        translated_count = len([m for m in zh_messages if "text_ja" in m])
    
    elif backend == 'gemini-batch':
        # Gemini Batch APIã§ç¿»è¨³
        if not api_key:
            print("ã‚¨ãƒ©ãƒ¼: Geminiã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ --api-key ã¾ãŸã¯ç’°å¢ƒå¤‰æ•° GOOGLE_API_KEY ãŒå¿…è¦ã§ã™", file=sys.stderr)
            sys.exit(1)
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ¢ãƒ‡ãƒ«èª¿æ•´
        if model == 'qwen2.5:7b':
            model = 'gemini-2.0-flash'
        
        translations = translate_with_gemini_batch(
            messages=messages,
            api_key=api_key,
            model=model,
            batch_size=batch_size,
            poll_interval=poll_interval,
            max_count=count
        )
        
        # ç¿»è¨³çµæœã‚’ãƒãƒ¼ã‚¸
        for m in messages:
            if m["id"] in translations:
                m["text_ja"] = translations[m["id"]]
        
        translated_count = len(translations)
    
    else:
        translated_count = 0
    
    # å‡ºåŠ›
    output_file.parent.mkdir(parents=True, exist_ok=True)
    with open(output_file, 'w', encoding='utf-8') as f:
        for m in messages:
            f.write(json.dumps(m, ensure_ascii=False) + '\n')
    
    return total_messages, translated_count


def process_directory(args):
    """
    ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    """
    input_dir = args.input_dir
    output_dir = args.output_dir
    
    # å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å­˜åœ¨ç¢ºèª
    if not input_dir.exists():
        print(f"ã‚¨ãƒ©ãƒ¼: å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {input_dir}", file=sys.stderr)
        sys.exit(1)
    
    # JSONLãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œå‡ºã—ã¦ã‚½ãƒ¼ãƒˆ
    jsonl_files = sorted(input_dir.glob('*.jsonl'))
    
    if not jsonl_files:
        print(f"ã‚¨ãƒ©ãƒ¼: {input_dir} ã« .jsonl ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“", file=sys.stderr)
        sys.exit(1)
    
    # æ—¥æ•°åˆ¶é™ï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‡¦ç†æ™‚ã¯ --count ã§æ—¥æ•°ã‚’åˆ¶é™ï¼‰
    if args.count is not None:
        original_count = len(jsonl_files)
        jsonl_files = jsonl_files[:args.count]
        print(f"å‡¦ç†å¯¾è±¡: {original_count}ãƒ•ã‚¡ã‚¤ãƒ«ä¸­ã€æœ€åˆã®{args.count}æ—¥åˆ†ã‚’å‡¦ç†")
    else:
        print(f"å‡¦ç†å¯¾è±¡: {len(jsonl_files)}ãƒ•ã‚¡ã‚¤ãƒ«")
    
    # API Keyå–å¾—ï¼ˆGeminiä½¿ç”¨æ™‚ï¼‰
    api_key = None
    if args.backend in ['gemini', 'gemini-batch']:
        api_key = args.api_key or os.environ.get("GOOGLE_API_KEY")
        if not api_key:
            print("ã‚¨ãƒ©ãƒ¼: Geminiã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ --api-key ã¾ãŸã¯ç’°å¢ƒå¤‰æ•° GOOGLE_API_KEY ãŒå¿…è¦ã§ã™", file=sys.stderr)
            sys.exit(1)
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # å„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†
    total_processed = 0
    total_translated = 0
    failed_files = []
    
    print(f"\nãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰: {args.backend}")
    print(f"ãƒ¢ãƒ‡ãƒ«: {args.model}")
    print("=" * 60)
    
    for idx, input_file in enumerate(jsonl_files, 1):
        # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        output_filename = input_file.stem + "_translated.jsonl"
        output_file = output_dir / output_filename
        
        print(f"\n[{idx}/{len(jsonl_files)}] {input_file.name} ã‚’å‡¦ç†ä¸­...")
        
        try:
            # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ï¼ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‡¦ç†æ™‚ã¯ count=None ã§å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ï¼‰
            processed, translated = process_single_file(
                input_file=input_file,
                output_file=output_file,
                backend=args.backend,
                model=args.model,
                api_key=api_key,
                detailed=args.detailed,
                timeout=args.timeout,
                batch_size=args.batch_size,
                poll_interval=args.poll_interval,
                count=None  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‡¦ç†æ™‚ã¯å„ãƒ•ã‚¡ã‚¤ãƒ«ã®å…¨ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†
            )
            
            total_processed += processed
            total_translated += translated
            
            print(f"  âœ“ å®Œäº†: {output_file.name} ({processed}ä»¶ä¸­{translated}ä»¶ç¿»è¨³)")
            
        except Exception as e:
            print(f"  âœ— ã‚¨ãƒ©ãƒ¼: {e}", file=sys.stderr)
            failed_files.append(input_file.name)
            continue
    
    # æœ€çµ‚çµæœ
    print("\n" + "=" * 60)
    print("å‡¦ç†å®Œäº†:")
    print(f"  å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(jsonl_files) - len(failed_files)}/{len(jsonl_files)}")
    print(f"  ç·ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {total_processed:,}ä»¶")
    print(f"  ç¿»è¨³ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {total_translated:,}ä»¶")
    
    if failed_files:
        print(f"\n  âš ï¸  å¤±æ•—ã—ãŸãƒ•ã‚¡ã‚¤ãƒ« ({len(failed_files)}ä»¶):")
        for filename in failed_files:
            print(f"    - {filename}")
    
    print(f"\nå‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")
    print("=" * 60)


def main():
    parser = argparse.ArgumentParser(description='ä¸­å›½èªãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ç¿»è¨³ã‚’è¿½åŠ ')
    
    # å…¥åŠ›/å‡ºåŠ›ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼ˆå˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã¾ãŸã¯ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼‰
    input_group = parser.add_mutually_exclusive_group(required=True)
    input_group.add_argument('--input', '-i', type=Path, help='å…¥åŠ›JSONLãƒ•ã‚¡ã‚¤ãƒ«')
    input_group.add_argument('--input-dir', type=Path, help='å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆæ—¥æ¯ã«åˆ†å‰²ã•ã‚ŒãŸJSONLãƒ•ã‚¡ã‚¤ãƒ«ï¼‰')
    
    output_group = parser.add_mutually_exclusive_group(required=True)
    output_group.add_argument('--output', '-o', type=Path, help='å‡ºåŠ›JSONLãƒ•ã‚¡ã‚¤ãƒ«')
    output_group.add_argument('--output-dir', type=Path, help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    
    parser.add_argument('--backend', '-b', default='none',
                       choices=['none', 'ollama', 'gemini', 'gemini-batch', 'export', 'merge'],
                       help='ç¿»è¨³ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ (gemini-batch: 50%%å‰²å¼•ã®ãƒãƒƒãƒAPI)')
    parser.add_argument('--model', '-m', default='qwen2.5:7b', help='ãƒ¢ãƒ‡ãƒ«å (Ollama: qwen2.5:7b, Gemini: gemini-2.0-flash ç­‰)')
    parser.add_argument('--batch-size', type=int, default=100, help='ãƒãƒƒãƒAPIã®1ãƒãƒƒãƒã‚ãŸã‚Šã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100)')
    parser.add_argument('--poll-interval', type=int, default=30, help='ãƒãƒƒãƒAPIã®ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèªé–“éš”ç§’æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 30)')
    parser.add_argument('--api-key', help='Google API Key (Geminiç”¨)ã€‚ç’°å¢ƒå¤‰æ•° GOOGLE_API_KEY ã‚‚ä½¿ç”¨å¯èƒ½')
    parser.add_argument('--timeout', type=int, default=180, help='Ollamaãƒªã‚¯ã‚¨ã‚¹ãƒˆã®ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆç§’æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 180)')
    parser.add_argument('--list-models', action='store_true', help='åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’è¡¨ç¤º')
    parser.add_argument('--translation-file', '-t', default=None,
                       help='ç¿»è¨³ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆexport/mergeç”¨ï¼‰')
    parser.add_argument('--detailed', action='store_true',
                       help='è©³ç´°ç¿»è¨³ãƒ¢ãƒ¼ãƒ‰ï¼ˆè¨€èªå­¦ç¿’å‘ã‘ã€å˜èªè§£èª¬ãƒ»ãƒ‹ãƒ¥ã‚¢ãƒ³ã‚¹åˆ†æãƒ»è¿”ä¿¡æ¡ˆã‚’å«ã‚€ï¼‰')
    parser.add_argument('--count', '--limit', '-n', type=int, default=None,
                       help='å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«: å‡¦ç†ã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°ã€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: å‡¦ç†ã™ã‚‹æ—¥æ•°ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰')

    args = parser.parse_args()
    
    # å…¥åŠ›/å‡ºåŠ›ã®æ•´åˆæ€§ãƒã‚§ãƒƒã‚¯
    if args.input and not args.output:
        parser.error("--input ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ --output ã‚‚æŒ‡å®šã—ã¦ãã ã•ã„")
    if args.input_dir and not args.output_dir:
        parser.error("--input-dir ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ --output-dir ã‚‚æŒ‡å®šã—ã¦ãã ã•ã„")
    if args.output and not args.input:
        parser.error("--output ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ --input ã‚‚æŒ‡å®šã—ã¦ãã ã•ã„")
    if args.output_dir and not args.input_dir:
        parser.error("--output-dir ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯ --input-dir ã‚‚æŒ‡å®šã—ã¦ãã ã•ã„")
    
    # ãƒ¢ãƒ‡ãƒ«ä¸€è¦§è¡¨ç¤º
    if args.list_models:
        models = get_available_models()
        if models:
            print("åˆ©ç”¨å¯èƒ½ãªOllamaãƒ¢ãƒ‡ãƒ«:")
            for model in models:
                print(f"  - {model}")
        else:
            print("Ollamaã«æ¥ç¶šã§ããªã„ã‹ã€ãƒ¢ãƒ‡ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‡¦ç†ãƒ¢ãƒ¼ãƒ‰
    if args.input_dir:
        process_directory(args)
        return
    
    # å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ãƒ¢ãƒ¼ãƒ‰
    api_key = args.api_key or os.environ.get("GOOGLE_API_KEY") if args.backend in ['gemini', 'gemini-batch'] else None
    
    print(f"å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {args.input}")
    print(f"ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰: {args.backend}")
    print(f"ãƒ¢ãƒ‡ãƒ«: {args.model}")
    
    total_messages, translated_count = process_single_file(
        input_file=args.input,
        output_file=args.output,
        backend=args.backend,
        model=args.model,
        api_key=api_key,
        detailed=args.detailed,
        timeout=args.timeout,
        batch_size=args.batch_size,
        poll_interval=args.poll_interval,
        count=args.count
    )
    
    print(f"\nå‡ºåŠ›å®Œäº†: {args.output}")
    print(f"ç·ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {total_messages:,}ä»¶")
    if translated_count > 0:
        print(f"ç¿»è¨³ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ•°: {translated_count:,}ä»¶")


if __name__ == '__main__':
    main()
